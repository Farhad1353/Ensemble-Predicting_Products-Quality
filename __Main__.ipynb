{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning project : Ensemble optimisation using hyperparameter and architecture search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1(A) - Describing the data set :\n",
    "\n",
    "The Data set has 11 features of different Red wines.\n",
    "Also the data has quality of the wines which will be considered as label in the project.\n",
    "\n",
    "There are two csv files related to the wine data set : \n",
    "\n",
    "1.Train file which will be used to implement training through various methods, also it will be used to test our models through validation sets. Further details will be followed later in the this notebook.\n",
    "\n",
    "2.Test file has exactly similar structure to the Train file and will be used for implementing the final stage of the project which is testing the reliability of the ensemble methods by checking the accuracy score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different stages in this project(Summary) :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A) Importing form the csv files :\n",
    "I have imorted the Data from the related CSV files(winequality-red-train.csv and winequality-red-test.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B) Implementing the First stage of Regression ensemble alogorithms :\n",
    "In this stage, I have trained several models and tested them(using validation sets) implementing RandomForest, AdaBoost and GradientBoost by tuning the HyperParameters and finding a rough guide of which HyperParameters give a better accuracy score for the this stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C) Importing the best results to csv files to be used in the 2nd stage of ensemle algorithms :\n",
    "In this stage, I have imported the best HyperParameters found on stage B of this project to three csv files :\n",
    "RandomForest_Regressor.csv, AdaBoost_Regressor.csv and GradientBoosting_Regressor.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D) Implementing Second stage of Regression ensemble algorithms :\n",
    "In this stage, I implemented a more precise tunning for the HyperParameters obtained from stage B, to get even a better accuracy by using the same three regression ensemble algorithms : RandomForest, AdaBoost and GradientBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E) Implementing the Final Stage for the ensemble algorithm :\n",
    "In this stage I have used StackingRegressor to combine the result from stage D and get the final model for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F) Applying Linear Regression to find he impact of the 11 feautures with presentation :\n",
    "In this stage, I have applied Linear Regression technique to get an image of how each feature is going to impact the quality f the Red wine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please find the flowchart below, which represents different stages in this project :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image \n",
    "Image(filename='./images/Project_Flowchart.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have created several functions to help with implementing and tunning the hyperparameters used in different methods.\n",
    "\n",
    "Please see below all the imported libraries at the begining of my coding(python standard, sklearn and my own libraries) which will be helping me to achieve the goal we have been tasked to get to in this project : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys                     # import libraries\n",
    "sys.path.append('.\\Library')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from tqdm import tqdm, trange\n",
    "import os\n",
    "                         \n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\\\n",
    "                            , VotingRegressor, StackingRegressor     # import sklearn libraries \n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from utils import rgb2gray, get_regression_data, visualise_regression_data# import our libraries\n",
    "from assigning_library import read_csv_files, split_train_validation, split_features_labels, read_param\n",
    "from regression_library import low_high_param, show_features_impact, plot_models, hyper_3_param_array, hyper_2_param_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1(B) - Load in the training and test splits of the dataset :\n",
    "\n",
    "Here, initially the two csv files are imported to variables using pandas.\n",
    "\n",
    "Then, the train csv will be split to header, training and validation sets.\n",
    "\n",
    "Also, the training and validation sets will be split further to seperate the wine features from the label(wine quality).\n",
    "\n",
    "Likewise the test csv, will be split to features and label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_file = \"./Data/winequality-red-train.csv\"  # assigning the train.csv file path to a variable\n",
    "path_test_file = \"./Data/winequality-red-test.csv\"  # assigning the test.csv file path to a variable\n",
    "\n",
    "my_train_File, my_test_File = read_csv_files(path_train_file, path_test_file)  # reading the csv files using pandas\n",
    "                                                                       \n",
    "header, X_train, X_validation, Y_train, Y_validation\\\n",
    "= split_train_validation(my_train_File, split_rate=0.28)  # splitting the train.csv to train and validation arrays\n",
    "\n",
    "X_test, Y_test = split_features_labels(my_test_File) # splitting the test.csv file to features and label arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1279 entries, 0 to 1278\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   fixed acidity         1279 non-null   float64\n",
      " 1   volatile acidity      1279 non-null   float64\n",
      " 2   citric acid           1279 non-null   float64\n",
      " 3   residual sugar        1279 non-null   float64\n",
      " 4   chlorides             1279 non-null   float64\n",
      " 5   free sulfur dioxide   1279 non-null   float64\n",
      " 6   total sulfur dioxide  1279 non-null   float64\n",
      " 7   density               1279 non-null   float64\n",
      " 8   pH                    1279 non-null   float64\n",
      " 9   sulphates             1279 non-null   float64\n",
      " 10  alcohol               1279 non-null   float64\n",
      " 11  quality               1279 non-null   int64  \n",
      "dtypes: float64(11), int64(1)\n",
      "memory usage: 120.0 KB\n"
     ]
    }
   ],
   "source": [
    "wine_df = pd.read_csv(\"./Data/winequality-red-train.csv\") # importing the train csv to a dataframe\n",
    "\n",
    "wine_df.drop(['Unnamed: 0'], axis=1, inplace = True) # Dropping the index column\n",
    "\n",
    "wine_df.info() # Here we can see the 11 features and the label(wine's quality) as mentioned above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2 - Fit the models on the Data Set and impove the models performance using ensembe methods : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type of models and HyperParameters tuning : \n",
    "\n",
    "All models used in this project are in regression form.\n",
    "\n",
    "RandomForest, AdaBoost and GradienBoost are the regression ensebmles which have been implemented in this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest(First Stage) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# assigning the starting, endingpoint and steps size taken to tune the HyperParameters\n",
    "\n",
    "n_estimator_strarting_point = 100   # Initial n_estimator will be used as a hyperparameter\n",
    "n_estimator_ending_point = 500      # The maximum n_estimator will be used as a hyperparameter\n",
    "n_estimator_step_size = 100         # The step taken while tunning the n_estimator hyperparameter\n",
    "\n",
    "max_features_starting_point = 2   # Initial max_features will be used as a hyperparameter\n",
    "max_features_ending_point = 10    # The maximum max_features will be used as a hyperparameter\n",
    "max_features_step_size = 1        # The step taken while tunning the max_features hyperparameter\n",
    "\n",
    "max_depth_starting_point = 3    # Initial max_depth will be used as a hyperparameter\n",
    "max_depth_ending_point = 40     # The maximum max_depth will be used as a hyperparameter\n",
    "max_depth_step_size = 4         # The step taken while tunning the max_depth hyperparameter\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    hyper_3_param_array function creates a 2 dimension array \n",
      "    which the rows are representing each model to be trained and \n",
      "    the columns are representing each of the 3 HyperParameters \n",
      "    chosen for the training\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(hyper_3_param_array.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForest_hyper_param_array = hyper_3_param_array(n_estimator_strarting_point, n_estimator_ending_point, n_estimator_step_size, \n",
    "                                          max_features_starting_point, max_features_ending_point, max_features_step_size,\n",
    "                                          max_depth_starting_point, max_depth_ending_point, max_depth_step_size)\n",
    "\n",
    "\n",
    "#adding the scores from all the models to an array to identify the best model\n",
    "RandomForest_score_array = np.ones((RandomForest_hyper_param_array.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 171/320 [01:33<01:43,  1.44it/s]"
     ]
    }
   ],
   "source": [
    "### training and testing(through validation sets) all the models\n",
    "for RandomForest_idx in trange(len(RandomForest_hyper_param_array)):\n",
    "    # assigning an instance of a model\n",
    "    randomforestregressor = RandomForestRegressor(n_estimators = int(RandomForest_hyper_param_array[RandomForest_idx,0]),\n",
    "                    max_features = RandomForest_hyper_param_array[RandomForest_idx,1], \n",
    "                    max_depth = RandomForest_hyper_param_array[RandomForest_idx,2])\n",
    "    \n",
    "    randomforestregressor.fit(X_train, Y_train)  # fitting the train arrays to each RandomForest model\n",
    "    \n",
    "    Y_prediction = np.around(randomforestregressor.predict(X_validation)) #predicting using the validation set  \n",
    "    \n",
    "    RandomForest_score_array[RandomForest_idx,0] = accuracy_score(Y_validation, Y_prediction) * 100 #accuaracy score for each model\n",
    "\n",
    "max_index = np.argmax(RandomForest_score_array[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Summary of the best HyperParameters which will be implemented in next stage and the best accuracy score\n",
    "\n",
    "os.system('cls')\n",
    "print(\"\\n               RandomForest performance \")\n",
    "print(\"            --------------------------------\")  # show the best model of RandomForest tested on validation set\n",
    "print(\"Best Regressor Score for RandomForest : {:.2f}%\".format(RandomForest_score_array[max_index,0])) \n",
    "print(\"Best Estimator number : \", RandomForest_hyper_param_array[max_index,0], \"\\nBest Features number : \"\n",
    "      , RandomForest_hyper_param_array[max_index,1], \"\\nbest_max_depth : \",RandomForest_hyper_param_array[max_index,2])\n",
    "\n",
    "randomforest_best_parameters = np.array([RandomForest_hyper_param_array[max_index,0]\n",
    "                              , RandomForest_hyper_param_array[max_index,1], RandomForest_hyper_param_array[max_index,2]])\n",
    "\n",
    "#Exporting the best HyperPatrameters to the relevant csv file for further stages\n",
    "np.savetxt('./Data/RandomForest_Regressor.csv', randomforest_best_parameters, fmt=\"%d\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Here we calculate how many subplots we need and how many models are allocated to each subplot\n",
    "\n",
    "number_of_subplots = int(((n_estimator_ending_point + n_estimator_step_size - n_estimator_strarting_point-1)/n_estimator_step_size))\n",
    "models_in_each_plot = int(len(RandomForest_hyper_param_array)/number_of_subplots)\n",
    "print(\"number of models in each subplot: \",models_in_each_plot, \"   number of subplots: \", number_of_subplots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The best model will be shown on the plot with a Red colour Star, slightly larger than other models which are represented by a triangle each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using scatter 3D plots to show the accuarcy of all the models\n",
    "\n",
    "for subplot_idx in range(number_of_subplots): # Loop through each subplot\n",
    "    \n",
    "    #deciding the size and type of the plots\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    #creating an array for x-axis\n",
    "    X = np.zeros((models_in_each_plot,))\n",
    "    for i in range(models_in_each_plot):\n",
    "        X[i] = RandomForest_hyper_param_array[i+subplot_idx*models_in_each_plot,1]\n",
    "    \n",
    "    #creating an array for y-axis\n",
    "    Y = np.zeros((models_in_each_plot,))\n",
    "    for i in range(models_in_each_plot):\n",
    "        Y[i] = RandomForest_hyper_param_array[i+subplot_idx*models_in_each_plot,2]\n",
    "    \n",
    "    #creating and array for z-axis\n",
    "    Z = np.zeros((models_in_each_plot,))\n",
    "    for i in range(models_in_each_plot):\n",
    "        Z[i] = RandomForest_score_array[i+subplot_idx*models_in_each_plot,0]\n",
    "    \n",
    "    #assigning the text and font size to the axis\n",
    "    ax.set_xlabel('Maximum Features', fontsize=14)\n",
    "    ax.set_ylabel('Maximum Depth', fontsize=14)\n",
    "    ax.set_zlabel('Accuracy Score', fontsize=14)\n",
    "    \n",
    "    #finding the best score\n",
    "    zmax = np.max(RandomForest_score_array[:,0])\n",
    "    \n",
    "    #setting the scatter plot to only show the models with high accuracy for better visibility\n",
    "    high_score =.9\n",
    "    \n",
    "    ax.set_title('n_estimator=%i' %(n_estimator_strarting_point+n_estimator_step_size*subplot_idx), fontsize =20)\n",
    "    \n",
    "    #Plotting the models with small triangle shape\n",
    "    ax.scatter(X[Z>zmax*high_score], Y[Z>zmax*high_score], Z[Z>zmax*high_score], marker='^',c=\"green\", s=100)\n",
    "    \n",
    "    #Plotting the best model with a Red star\n",
    "    ax.scatter(X[Z==zmax], Y[Z==zmax], Z[Z==zmax], marker='*',c=\"red\", s=500)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost(First Stage) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning the starting, endingpoint and steps size taken to tune the HyperParameters\n",
    "\n",
    "n_estimator_strarting_point = 10   # Initial n_estimator will be used as a hyperparameter\n",
    "n_estimator_ending_point = 100      # The maximum n_estimator will be used as a hyperparameter\n",
    "n_estimator_step_size = 10         # The step taken while tunning the n_estimator hyperparameter\n",
    "\n",
    "\n",
    "#The learning rate input are multiplied by 100 to keep it as integer in the nested Loop(So, 7 is learning rate of 0.07)\n",
    "learning_rate_starting_point = 7   # Initial learning_rate will be used as a hyperparameter\n",
    "learning_rate_ending_point = 100    # The maximum learning_rate will be used as a hyperparameter\n",
    "learning_rate_step_size = 3        # The step taken while tunning the learning_rate hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hyper_2_param_array.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AdaBoost_hyper_param_array = hyper_2_param_array(n_estimator_strarting_point, n_estimator_ending_point, n_estimator_step_size, \n",
    "                                          learning_rate_starting_point, learning_rate_ending_point, learning_rate_step_size)\n",
    "\n",
    "\n",
    "#adding the scores from all the models to an array to identify the best model\n",
    "AdaBoost_score_array = np.ones((AdaBoost_hyper_param_array.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### training and testing(through validation sets) all the models\n",
    "for AdaBoost_idx in trange(len(AdaBoost_hyper_param_array)):\n",
    "    # assigning an instance of a model\n",
    "    AdaBoostregressor = AdaBoostRegressor(n_estimators = int(AdaBoost_hyper_param_array[AdaBoost_idx,0]),\n",
    "                        learning_rate = AdaBoost_hyper_param_array[AdaBoost_idx,1]/100)\n",
    "    \n",
    "    AdaBoostregressor.fit(X_train, Y_train)  # fitting the train arrays to each AdaBoost model\n",
    "    \n",
    "    Y_prediction = np.around(AdaBoostregressor.predict(X_validation)) #predicting using the validation set  \n",
    "    \n",
    "    AdaBoost_score_array[AdaBoost_idx,0] = accuracy_score(Y_validation, Y_prediction) * 100 #accuaracy score for each model\n",
    "\n",
    "max_index = np.argmax(AdaBoost_score_array[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary of the best HyperParameters which will be implemented in next stage and the best accuracy score\n",
    "\n",
    "os.system('cls')\n",
    "print(\"\\n               AdaBoost performance \")\n",
    "print(\"            --------------------------------\")  # show the best model of AdaBoost tested on validation set\n",
    "print(\"Best Regressor Score for AdaBoost : {:.2f}%\".format(AdaBoost_score_array[max_index,0])) \n",
    "print(\"Best Estimator number : \", AdaBoost_hyper_param_array[max_index,0], \"\\nBest Learning_rate : \"\n",
    "      , AdaBoost_hyper_param_array[max_index,1]/100)\n",
    "\n",
    "AdaBoost_best_parameters = np.array([AdaBoost_hyper_param_array[max_index,0]\n",
    "                              , AdaBoost_hyper_param_array[max_index,1]])\n",
    "\n",
    "#Exporting the best HyperPatrameters to the relevant csv file for further stages\n",
    "np.savetxt('./Data/AdaBoost_Regressor.csv', AdaBoost_best_parameters, fmt=\"%d\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we calculate how many models are allocated to the plot\n",
    "\n",
    "number_of_estimators = int(((n_estimator_ending_point + n_estimator_step_size - n_estimator_strarting_point-1)/n_estimator_step_size))\n",
    "total_models = int(len(AdaBoost_hyper_param_array))\n",
    "print(\"Number of models : \",total_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The best model will be shown on the plot with a Red colour Star, slightly larger than other models which are represented by a triangle each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using scatter 3D plots to show the accuarcy of all the models\n",
    "\n",
    "#deciding the size and type of the plots\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "#creating an array for x-axis\n",
    "X = np.zeros((total_models,))\n",
    "for i in range(total_models):\n",
    "    X[i] = AdaBoost_hyper_param_array[i,0]\n",
    "    \n",
    "#creating an array for y-axis\n",
    "Y = np.zeros((total_models,))\n",
    "for i in range(total_models):\n",
    "    Y[i] = AdaBoost_hyper_param_array[i,1]/100\n",
    "    \n",
    "#creating and array for z-axis\n",
    "Z = np.zeros((total_models,))\n",
    "for i in range(total_models):\n",
    "    Z[i] = AdaBoost_score_array[i,0]\n",
    "    \n",
    "#assigning the text and font size to the axis\n",
    "ax.set_xlabel('n_estimtors', fontsize=14)\n",
    "ax.set_ylabel('Learning_rate', fontsize=14)\n",
    "ax.set_zlabel('Accuracy Score', fontsize=14)\n",
    "    \n",
    "#finding the best score\n",
    "zmax = np.max(AdaBoost_score_array[:,0])\n",
    "    \n",
    "#setting the scatter plot to only show the models with high accuracy for better visibility\n",
    "high_score =.9\n",
    "    \n",
    "ax.set_title('Accuracy rate for AdaBoost models', fontsize =20)\n",
    "    \n",
    "#Plotting the models with small triangle shape\n",
    "ax.scatter(X[Z>zmax*high_score], Y[Z>zmax*high_score], Z[Z>zmax*high_score], marker='^',c=\"blue\", s=150)\n",
    "    \n",
    "#Plotting the best model with a Red star\n",
    "ax.scatter(X[Z==zmax], Y[Z==zmax], Z[Z==zmax], marker='*',c=\"red\", s=800)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientBoost(First Stage) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning the starting, endingpoint and steps size taken to tune the HyperParameters\n",
    "\n",
    "n_estimator_strarting_point = 60   # Initial n_estimator will be used as a hyperparameter\n",
    "n_estimator_ending_point = 130      # The maximum n_estimator will be used as a hyperparameter\n",
    "n_estimator_step_size = 10         # The step taken while tunning the n_estimator hyperparameter\n",
    "\n",
    "#The learning rate input are multiplied by 100 to keep it as integer in the nested Loop(So, 3 is learning rate of 0.03)\n",
    "learning_rate_starting_point = 3   # Initial learning_rate will be used as a hyperparameter\n",
    "learning_rate_ending_point = 50    # The maximum learning_rate will be used as a hyperparameter\n",
    "learning_rate_step_size = 5        # The step taken while tunning the learning_rate hyperparameter\n",
    "\n",
    "max_depth_starting_point = 3    # Initial max_depth will be used as a hyperparameter\n",
    "max_depth_ending_point = 10     # The maximum max_depth will be used as a hyperparameter\n",
    "max_depth_step_size = 2         # The step taken while tunning the max_depth hyperparameter\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hyper_3_param_array.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GradientBoost_hyper_param_array = hyper_3_param_array(n_estimator_strarting_point, n_estimator_ending_point, n_estimator_step_size, \n",
    "                                          learning_rate_starting_point, learning_rate_ending_point, learning_rate_step_size,\n",
    "                                          max_depth_starting_point, max_depth_ending_point, max_depth_step_size)\n",
    "\n",
    "\n",
    "#adding the scores from all the models to an array to identify the best model\n",
    "GradientBoost_score_array = np.ones((GradientBoost_hyper_param_array.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### training and testing(through validation sets) all the models\n",
    "for GradientBoost_idx in trange(len(GradientBoost_hyper_param_array)):\n",
    "    # assigning an instance of a model\n",
    "    gradientBoostregressor = GradientBoostingRegressor(n_estimators = int(GradientBoost_hyper_param_array[GradientBoost_idx,0]),\n",
    "                    learning_rate = GradientBoost_hyper_param_array[GradientBoost_idx,1]/100, \n",
    "                    max_depth = GradientBoost_hyper_param_array[GradientBoost_idx,2])\n",
    "    \n",
    "    gradientBoostregressor.fit(X_train, Y_train)  # fitting the train arrays to each GradientBoost model\n",
    "    \n",
    "    Y_prediction = np.around(gradientBoostregressor.predict(X_validation)) #predicting using the validation set  \n",
    "    \n",
    "    GradientBoost_score_array[GradientBoost_idx,0] = accuracy_score(Y_validation, Y_prediction) * 100 #accuaracy score for each model\n",
    "\n",
    "max_index = np.argmax(GradientBoost_score_array[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary of the best HyperParameters which will be implemented in next stage and the best accuracy score\n",
    "\n",
    "os.system('cls')\n",
    "print(\"\\n               GradientBoost performance \")\n",
    "print(\"            --------------------------------\")  # show the best model of GradientBoost tested on validation set\n",
    "print(\"Best Regressor Score for GradientBoost : {:.2f}%\".format(GradientBoost_score_array[max_index,0])) \n",
    "print(\"Best Estimator number : \", GradientBoost_hyper_param_array[max_index,0], \"\\nBest Learning_rate : \"\n",
    "      , GradientBoost_hyper_param_array[max_index,1]/100, \"\\nbest_max_depth : \",GradientBoost_hyper_param_array[max_index,2])\n",
    "\n",
    "GradientBoost_best_parameters = np.array([GradientBoost_hyper_param_array[max_index,0]\n",
    "                              , GradientBoost_hyper_param_array[max_index,1], GradientBoost_hyper_param_array[max_index,2]])\n",
    "\n",
    "#Exporting the best HyperPatrameters to the relevant csv file for further stages\n",
    "np.savetxt('./Data/GradientBoosting_Regressor.csv', GradientBoost_best_parameters, fmt=\"%d\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we calculate how many subplots we need and how many models are allocated to each subplot\n",
    "\n",
    "number_of_subplots = int(((n_estimator_ending_point + n_estimator_step_size - n_estimator_strarting_point-1)/n_estimator_step_size))\n",
    "models_in_each_plot = int(len(GradientBoost_hyper_param_array)/number_of_subplots)\n",
    "print(\"number of models in each subplot: \",models_in_each_plot, \"   number of subplots: \", number_of_subplots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The best model will be shown on the plot with a Red colour Star, slightly larger than other models which are represented by a triangle each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using scatter 3D plots to show the accuarcy of all the models\n",
    "\n",
    "for subplot_idx in range(number_of_subplots): # Loop through each subplot\n",
    "    \n",
    "    #deciding the size and type of the plots\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    #creating an array for x-axis\n",
    "    X = np.zeros((models_in_each_plot,))\n",
    "    for i in range(models_in_each_plot):\n",
    "        X[i] = GradientBoost_hyper_param_array[i+subplot_idx*models_in_each_plot,1]/100\n",
    "    \n",
    "    #creating an array for y-axis\n",
    "    Y = np.zeros((models_in_each_plot,))\n",
    "    for i in range(models_in_each_plot):\n",
    "        Y[i] = GradientBoost_hyper_param_array[i+subplot_idx*models_in_each_plot,2]\n",
    "    \n",
    "    #creating and array for z-axis\n",
    "    Z = np.zeros((models_in_each_plot,))\n",
    "    for i in range(models_in_each_plot):\n",
    "        Z[i] = GradientBoost_score_array[i+subplot_idx*models_in_each_plot,0]\n",
    "    \n",
    "    #assigning the text and font size to the axis\n",
    "    ax.set_xlabel('Learning_rate', fontsize=14)\n",
    "    ax.set_ylabel('Maximum Depth', fontsize=14)\n",
    "    ax.set_zlabel('Accuracy Score', fontsize=14)\n",
    "    \n",
    "    #finding the best score\n",
    "    zmax = np.max(GradientBoost_score_array[:,0])\n",
    "    \n",
    "    #setting the scatter plot to only show the models with high accuracy for better visibility\n",
    "    high_score =.9\n",
    "    \n",
    "    ax.set_title('n_estimator=%i' %(n_estimator_strarting_point+n_estimator_step_size*subplot_idx), fontsize =20)\n",
    "    \n",
    "    #Plotting the models with small triangle shape\n",
    "    ax.scatter(X[Z>zmax*high_score], Y[Z>zmax*high_score], Z[Z>zmax*high_score], marker='^',c=\"orange\", s=100)\n",
    "    \n",
    "    #Plotting the best model with a Red star\n",
    "    ax.scatter(X[Z==zmax], Y[Z==zmax], Z[Z==zmax], marker='*',c=\"red\", s=500)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest(2nd Stage):\n",
    "\n",
    "At this stage 27 different RandomForest ensembles has been trained and tested(using validation sets) using 3 different HyperParameters : n_estimator, max_features and max_depth.\n",
    "\n",
    "The HyperParameters have been read automatically from RandomForest_reg.csv, which has been obtained in the first stage and used for a much narrow range in this stage.\n",
    "\n",
    "Please note, we can manually change the HyperParameters obtained from 1st stage in the csv file if we prefer it to the automated method.\n",
    "\n",
    "n_estimators used: {50, 100, 150}.\n",
    "\n",
    "max_features used: {1, 2, 3}\n",
    "\n",
    "Max_depth used : {25, 27, 29}\n",
    "\n",
    "Please see below the best performance found for RandomForest ensembles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_regr_score_randomforest = 0  # variable which shows the best scoring RandomForest model\n",
    "progress_randomforest = 0   # variable which indicates how far have we processed our RandomForest models\n",
    "regr_score_randomforest = 0 # variable which shows the score for each of the Random Forest model\n",
    "\n",
    "n_estimator_midpoint, max_features_midpoint, max_depth_midpoint\\\n",
    "                = read_param(\"./Data/RandomForest_reg.csv\", 3) # reading the parameters from 1st stage of RandomForest\n",
    "n_estimator_step_size = 50 # defining the step change of n_estimator moving from one model to another\n",
    "n_estimator_strarting_point, n_estimator_ending_point = low_high_param\\\n",
    "(n_estimator_midpoint, n_estimator_step_size, 3) # finding lowest and highest values for n_estimators of RandomForest\n",
    "\n",
    "max_features_step_size = 1 # defining the step change of max_features moving from one model to another\n",
    "max_features_starting_point, max_features_ending_point = low_high_param\\\n",
    "(max_features_midpoint,max_features_step_size, 3) # finding lowest and highest values for max_features of RandomForest\n",
    "    \n",
    "max_depth_step_size = 2 # defining the step change of max_depth moving from one model to another\n",
    "max_depth_starting_point, max_depth_ending_point = low_high_param\\\n",
    "(max_depth_midpoint,max_depth_step_size, 3) # finding lowest and highest values for depth_features of RandomForest\n",
    "\n",
    "randomforest_models = np.zeros((27,2)) # defining an array for accuracy scores\n",
    "model_idx = 0     # index for each model                           \n",
    "best_model_score = 0    # variable for the best score\n",
    "\n",
    "                ### training and testing(through validation sets) all the RandomForest models\n",
    "for n_estimator_idx in range(n_estimator_strarting_point, n_estimator_ending_point, n_estimator_step_size):             \n",
    "    for max_features_idx in range(max_features_starting_point, max_features_ending_point, max_features_step_size):      \n",
    "        for max_depth_idx in range(max_depth_starting_point, max_depth_ending_point, max_depth_step_size):  \n",
    "            os.system('cls')\n",
    "            print(\"Random Forest Progress : \",int(progress_randomforest),\"%\")  # showing the progress of the process\n",
    "            print(\"Number of estimator of {} Maximum features of {} and Maximum depth of {} \\\n",
    "                    gives accuracy score of {:.2f}%\".format(n_estimator_idx, max_features_idx, \\\n",
    "                    max_depth_idx, regr_score_randomforest*100))  # score of one single RandomForest model\n",
    "            print(\"Best Score so far : \", round(best_regr_score_randomforest*100,2), \"%\")\n",
    "            progress_randomforest += 100/27\n",
    "            randomforestregressor = RandomForestRegressor(n_estimators = n_estimator_idx,  \\\n",
    "                max_features = max_features_idx, max_depth = max_depth_idx) # assigning an instance of a RandomForest\n",
    "            randomforestregressor.fit(X_train, Y_train)  # fitting the train arrays to each RandomForest model\n",
    "            Y_prediction = np.around(randomforestregressor.predict(X_validation)) #predicting using the validation set\n",
    "            regr_score_randomforest = accuracy_score(Y_validation, Y_prediction) #accuaracy score for each model\n",
    "            randomforest_models[model_idx, 1] = regr_score_randomforest * 100 # saving the score for each model\n",
    "            randomforest_models[model_idx, 0] = model_idx                 # saving the index for each model\n",
    "            if regr_score_randomforest > best_regr_score_randomforest: # assessing if the new score is better \n",
    "                best_n_estimators_randomforest = n_estimator_idx      # than the previous best score\n",
    "                best_max_features_randomforest = max_features_idx \n",
    "                best_max_depth_randomforest = max_depth_idx  \n",
    "                best_regr_score_randomforest = regr_score_randomforest\n",
    "                best_model_score = randomforest_models[model_idx, 1]\n",
    "                best_model_idx = randomforest_models[model_idx, 0]\n",
    "            model_idx+=1 \n",
    "os.system('cls')\n",
    "print(\"\\n               RandomForest performance \")\n",
    "print(\"            --------------------------------\") # show the best model of RandomForest tested on validation set\n",
    "print(\"Best Regressor Score for Random Forest : {:.2f}%\".format(best_regr_score_randomforest*100)) \n",
    "print(\"Best Estimator number : \", best_n_estimators_randomforest, \"\\nBest Features number : \"\\\n",
    "      , best_max_features_randomforest, \"\\nbest_max_depth : \",best_max_depth_randomforest)\n",
    "\n",
    "plot_models(randomforest_models[:,0], randomforest_models[:,1], \"RandomForest-Models\", 'g') # Plot all the models\n",
    "\n",
    "print(\"Best model : model \",int(best_model_idx), \" with score of \", round(best_model_score,2), \"%\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost(First Stage):\n",
    "\n",
    "At first stage 279 different AdaBoost ensembles has been trained and tested(using validation sets) using 2 different HyperParameters : n_estimator and learning_rate.\n",
    "\n",
    "n_estimators used: {10, 20,..., 90}.\n",
    "\n",
    "learning_rate used: {0.07, 0.10, 0.13,..., 0.97}\n",
    "\n",
    "Please see below the best performance found for AdaBoost ensembles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image \n",
    "Image(filename='./images/AB_res.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image \n",
    "Image(filename='./images/AB_PLOT.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best result: model 191 out of 279 models tested with score of 64.06 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost(2nd Stage):\n",
    "\n",
    "At this stage 25 different AdaBoost ensembles has been trained and tested(using validation sets) using 2 different HyperParameters : n_estimator and learning_rate.\n",
    "\n",
    "The HyperParameters have been read automatically from AdaBoost_reg.csv, which has been obtained in the first stage and used for a much narrow range in this stage.\n",
    "\n",
    "Please note, we can manually change the HyperParameters obtained from 1st stage in the csv file if we prefer it to the automated method.\n",
    "\n",
    "n_estimators used: {62, 66, 70, 74, 78}.\n",
    "\n",
    "Learning_rate used: {0.16, 0.19, 0.22, 0.25, 0.28}\n",
    "\n",
    "Please see below the best performance found for AdaBoost ensembles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_regr_score_adaboost = 0 # variable which shows the best scoring AdaBoost model\n",
    "progress_adaboost = 0  # variable which indicates how far have we processed our AdaBoost models\n",
    "regr_score_adaboost = 0 # variable which shows the score for each of the AdaBoost model\n",
    "\n",
    "n_estimator_midpoint, learning_rate_midpoint = read_param(\"./Data/AdaBoost_reg.csv\", 2) # reading the parameters from 1st stage of AdaBoost\n",
    "n_estimator_step_size = 4 # defining the step change of n_estimator moving from one model to another\n",
    "n_estimator_strarting_point, n_estimator_ending_point = low_high_param\\\n",
    "(n_estimator_midpoint, n_estimator_step_size, 2) # finding lowest and highest values for n_estimators of AdaBoost\n",
    "\n",
    "learning_rate_step_size = 3 # defining the step change of max_features moving from one model to another\n",
    "learning_rate_starting_point, learning_rate_ending_point = low_high_param\\\n",
    "(learning_rate_midpoint, learning_rate_step_size, 2) # finding lowest and highest values for learning_rate of AdaBoost\n",
    "adaboost_models = np.zeros((25,2)) # defining an array for accuracy scores\n",
    "model_idx = 0     # index for each model                           \n",
    "best_model_score = 0    # variable for the best score\n",
    "\n",
    "                ### training and testing(through validation sets) all the AdaBoost models\n",
    "for n_estimator_idx in range(n_estimator_strarting_point, n_estimator_ending_point, n_estimator_step_size):             \n",
    "    for learning_rate_idx in range(learning_rate_starting_point, learning_rate_ending_point, learning_rate_step_size):       \n",
    "        os.system('cls')\n",
    "        print(\"AdaBoost Progress : \",int(progress_adaboost),\"%\")  # showing the progress of the process\n",
    "        print(\"Number of estimator of {}      Learning Rate of {} \\\n",
    "        gives accuracy score of {:.2f}%\".format(n_estimator_idx, learning_rate_idx/100,\\\n",
    "        regr_score_adaboost*100))  # score of one single AdaBoost model\n",
    "        print(\"Best Score so far : \", round(best_regr_score_adaboost*100,2), \"%\")\n",
    "        progress_adaboost += 100/25\n",
    "        adaboostregressor = AdaBoostRegressor(n_estimators = n_estimator_idx,\\\n",
    "        learning_rate = learning_rate_idx/100) # assigning an instance of a AdaBoost\n",
    "        adaboostregressor.fit(X_train, Y_train)  # fitting the train arrays to each AdaBoost model\n",
    "        Y_prediction = np.around(adaboostregressor.predict(X_validation)) #predicting using the validation set\n",
    "        regr_score_adaboost = accuracy_score(Y_validation, Y_prediction) #accuaracy score for each model\n",
    "        adaboost_models[model_idx, 1] = regr_score_adaboost * 100 # saving the score for each model\n",
    "        adaboost_models[model_idx, 0] = model_idx    # saving the index for each model\n",
    "        if regr_score_adaboost > best_regr_score_adaboost: # assessing if the new score is better \n",
    "            best_n_estimators_adaboost = n_estimator_idx      # than the previous best score\n",
    "            best_learning_rate_adaboost = learning_rate_idx  \n",
    "            best_regr_score_adaboost = regr_score_adaboost\n",
    "            best_model_score = adaboost_models[model_idx, 1]\n",
    "            best_model_idx = adaboost_models[model_idx, 0]\n",
    "        model_idx+=1\n",
    "os.system('cls')\n",
    "print(\"\\n                  AdaBoost performance \")\n",
    "print(\"            --------------------------------\") # show the best model of AdaBoost tested on validation set\n",
    "print(\"Best Regressor Score for AdaBoost : {:.2f}%\".format(best_regr_score_adaboost*100)) \n",
    "print(\"Best Estimator number : \", best_n_estimators_adaboost, \"\\nBest Learning rate : \"\\\n",
    "      , best_learning_rate_adaboost/100)\n",
    "\n",
    "plot_models(adaboost_models[:,0], adaboost_models[:,1], \"AdaBoost-Models\", 'b') # Plot all the models\n",
    "\n",
    "print(\"Best model : model \",int(best_model_idx), \" with score of \", round(best_model_score,2), \"%\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientBoosting(First Stage):\n",
    "    \n",
    "At first stage 280 different GradientBoosting ensembles has been trained and tested(using validation sets) using 3 different HyperParameters : n_estimator, learning_rate and max_depth.\n",
    "\n",
    "n_estimators used: {60, 70,..., 120}.\n",
    "\n",
    "learning_rate used: {0.03, 0.08, 0.13, ..., 0.48}\n",
    "\n",
    "Max_depth used : {3, 5, 7, 9}\n",
    "\n",
    "Please see below the best performance found for GradientBoosting ensembles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image \n",
    "Image(filename='./images/GB_res.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image \n",
    "Image(filename='./images/GB_PLOT.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best result: model 10 out of 280 models tested, with score of 84.38 %."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientBoosting(2nd Stage):\n",
    "    \n",
    "At this stage 27 different GradientBoosting ensembles has been trained and tested(using validation sets) using 3 different HyperParameters : n_estimator, learning_rate and max_depth.\n",
    "\n",
    "The HyperParameters have been read automatically from GradientBoosting_reg.csv, which has been obtained in the first stage and used for a much narrow range in this stage.\n",
    "\n",
    "Please note, we can manually change the HyperParameters obtained from 1st stage in the csv file if we prefer it to the automated method.\n",
    "\n",
    "n_estimators used: {55, 60, 65}.\n",
    "\n",
    "learning_rate used: {0.11, 0.13, 0.15}\n",
    "\n",
    "Max_depth used : {6, 7, 8}\n",
    "\n",
    "Please see below the best performance found for GradientBoosting ensembles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_regr_score_gradientboosting = 0  # variable which shows the best scoring Gradientboosting model\n",
    "progress_gradientboosting = 0   # variable which indicates how far have we processed our Gradientboosting models\n",
    "regr_score_gradientboosting = 0 # variable which shows the score for each of the Gradientboosting model\n",
    "\n",
    "n_estimator_midpoint, learning_rate_midpoint, max_depth_midpoint\\\n",
    "                = read_param(\"./Data/Gradientboosting_reg.csv\", 3) # reading the parameters from 1st stage of Gradientboosting\n",
    "n_estimator_step_size = 5 # defining the step change of n_estimator moving from one model to another\n",
    "n_estimator_strarting_point, n_estimator_ending_point = low_high_param\\\n",
    "(n_estimator_midpoint, n_estimator_step_size, 3) # finding lowest and highest values for n_estimators of Gradientboosting\n",
    "\n",
    "learning_rate_step_size = 2 # defining the step change of learning_rate moving from one model to another\n",
    "learning_rate_starting_point, learning_rate_ending_point = low_high_param\\\n",
    "(learning_rate_midpoint,learning_rate_step_size, 3) # finding lowest and highest values for learning_rate of Gradientboosting\n",
    "    \n",
    "max_depth_step_size = 1 # defining the step change of max_depth moving from one model to another\n",
    "max_depth_starting_point, max_depth_ending_point = low_high_param\\\n",
    "(max_depth_midpoint,max_depth_step_size, 3) # finding lowest and highest values for depth_features of Gradientboosting\n",
    "gradientboost_models = np.zeros((27,2)) # defining an array for accuracy scores\n",
    "model_idx = 0     # index for each model                           \n",
    "best_model_score = 0    # variable for the best score    \n",
    "                ### training and testing(through validation sets) all the Gradientboosting models\n",
    "for n_estimator_idx in range(n_estimator_strarting_point, n_estimator_ending_point, n_estimator_step_size):             \n",
    "    for learning_rate_idx in range(learning_rate_starting_point, learning_rate_ending_point, learning_rate_step_size):      \n",
    "        for max_depth_idx in range(max_depth_starting_point, max_depth_ending_point, max_depth_step_size):  \n",
    "            os.system('cls')\n",
    "            print(\"GradientBoosting Progress : \",int(progress_gradientboosting),\"%\")  # showing the progress of the process\n",
    "            print(\"Number of estimator of {}     Learning rate of {} and Maximum depth of {} \\\n",
    "                    gives accuracy score of {:.2f}%\".format(n_estimator_idx, learning_rate_idx/100, \\\n",
    "                    max_depth_idx, regr_score_gradientboosting*100))  # score of one single Gradientboosting model\n",
    "            print(\"Best Score so far : \", round(best_regr_score_gradientboosting*100,2), \"%\")\n",
    "            progress_gradientboosting += 100/27\n",
    "            gradientboostingregressor = GradientBoostingRegressor(n_estimators = n_estimator_idx,  \\\n",
    "                learning_rate = learning_rate_idx/100, max_depth = max_depth_idx) # assigning an instance of a Gradientboosting\n",
    "            gradientboostingregressor.fit(X_train, Y_train)  # fitting the train arrays to each Gradientboosting model\n",
    "            Y_prediction = np.around(gradientboostingregressor.predict(X_validation)) #predicting using the validation set\n",
    "            regr_score_gradientboosting = accuracy_score(Y_validation, Y_prediction) #accuaracy score for each model\n",
    "            gradientboost_models[model_idx, 1] = regr_score_gradientboosting * 100 # saving the score for each model\n",
    "            gradientboost_models[model_idx, 0] = model_idx       # saving the index for each model\n",
    "            if regr_score_gradientboosting > best_regr_score_gradientboosting: # assessing if the new score is better \n",
    "                best_n_estimators_gradientboosting = n_estimator_idx      # than the previous best score\n",
    "                best_learning_rate_gradientboosting = learning_rate_idx \n",
    "                best_max_depth_gradientboosting = max_depth_idx  \n",
    "                best_regr_score_gradientboosting = regr_score_gradientboosting\n",
    "                best_model_score = gradientboost_models[model_idx, 1]\n",
    "                best_model_idx = gradientboost_models[model_idx, 0]\n",
    "            model_idx+=1 \n",
    "os.system('cls')\n",
    "print(\"\\n             GradientBoosting performance \")\n",
    "print(\"            --------------------------------\") # show the best model of Gradientboosting tested on validation set\n",
    "print(\"Best Regressor Score for Random Forest : {:.2f}%\".format(best_regr_score_gradientboosting*100)) \n",
    "print(\"Best Estimator number : \", best_n_estimators_gradientboosting, \"\\nBest Learning rate : \"\\\n",
    "      , best_learning_rate_gradientboosting/100, \"\\nbest_max_depth : \",best_max_depth_gradientboosting)\n",
    "\n",
    "plot_models(gradientboost_models[:,0], gradientboost_models[:,1], \"GradientBoost-Models\", 'r') # Plot all the models\n",
    "\n",
    "print(\"Best model : model \",int(best_model_idx), \" with score of \", round(best_model_score,2), \"%\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Algorithm to find the best combination of models and hyperparameters\n",
    "\n",
    "At final stage of ensembling, all three ensembles models : RandomForest, AdaBoost and GradientBoosting has been combined using StackingRegressor.\n",
    "\n",
    "Stacking allows to use the strength of each individual estimator by using their output as input of a final estimator(A regressor which will be used to combine the base estimators).\n",
    "\n",
    "In Parts 2&3 the best HyperParameters have been found for each of ensemble model after using 1000s of estimotors.\n",
    "\n",
    "The best HyperParametersfound in previous parts are being use in the three ensemle models to combine them using the StackingRegressor method.\n",
    "\n",
    "Please see below the accuracy score for applying and running the code for  the StackingRegressor method on the validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "######### Assigning the best performed HyperParameters in the RandomForest, AdaBoost and GradientBoosting ##########\n",
    "reg1 = RandomForestRegressor(n_estimators = best_n_estimators_randomforest\\\n",
    "    ,  max_features = best_max_features_randomforest, max_depth = best_max_depth_randomforest)\n",
    "reg2 = AdaBoostRegressor(n_estimators = best_n_estimators_adaboost,  learning_rate = best_learning_rate_adaboost/100)\n",
    "reg3 = GradientBoostingRegressor(n_estimators = best_n_estimators_gradientboosting\\\n",
    "    ,  learning_rate = best_learning_rate_gradientboosting/100, max_depth = best_max_depth_gradientboosting)\n",
    "\n",
    "os.system('cls')\n",
    "print(\"\\n               StackingRegressor performance on validation sets \")\n",
    "print(\"            ----------------------------------------------------------\")\n",
    "stackingregressor = StackingRegressor(estimators=[('rf1', reg1), ('ad1', reg2), ('gb1', reg3)])\n",
    "stackingregressor = stackingregressor.fit(X_train, Y_train)        ### Fitting the StackingRegressor model #####  \n",
    "Y_pred = np.around(stackingregressor.predict(X_validation)) ##### Predicting the Labels based on the validation features #####\n",
    "ensemble_score = accuracy_score(Y_validation, Y_pred)  ##### Calulating the accuracy of each model ####\n",
    "\n",
    "print(\"Regressor Score for StackingRegressor : {:.2f}%\".format(ensemble_score*100))  ## showing the ensemble score ##\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please see below applying the StackingRegressor method on the test sets :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n              StackingRegressor performance on test sets \")\n",
    "print(\"            ---------------------------------------------------\")\n",
    "\n",
    "Y_pred = np.around(stackingregressor.predict(X_test)) ##### Predicting the Labels based on the test features #####\n",
    "ensemble_score = accuracy_score(Y_test, Y_pred)  ##### Calulating the accuracy of each model ####\n",
    "\n",
    "print(\"Regressor Score for StackingRegressor : {:.2f}%\".format(ensemble_score*100))  ## showing the ensemble score ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 Presenting the results: \n",
    "\n",
    "Please see the graphs in all 3 ensemble models(1st and 2nd stage) in Parts 2 and 3 of this notebook.\n",
    "\n",
    "AdaBoost is extremely sensitive to Noisy data and outliers. So, this could have an impact in getting a lower accuracy result in comparison to GradientBoost and RandomForest.\n",
    "\n",
    "The StackingRegressor has accuracy score in range of (80%-85%) on the validation sets, however the accuracy score on the test sets is in range of (65%-70%). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6  Most effective features(Ranked) :\n",
    "\n",
    "LinearRegression model has been applied to training set to obtain the most affective features.\n",
    "\n",
    "The result is ranked by most effective(which can have negative and positive impact in the quality of the wine we are testing) features impacting the red wine quality.\n",
    "\n",
    "Alchohol and Sulphates respectively have the highest positve impact on the quality of the Red wine.\n",
    "\n",
    "The Total sulfur dioxide, volatile acidity and cholrides have negative impact on the quality of the Red wine.\n",
    "\n",
    "Other six features do not seem to have a great impact on the quality of the Red wine.\n",
    "\n",
    "Please run the code below to see the result :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.system('cls')\n",
    "linearregression = LinearRegression()\n",
    "linearregression.fit(X_train, Y_train) ### Fitting the LinearRegression model #####\n",
    "\n",
    "Y_pred = np.around(linearregression.predict(X_validation))  ##### Predicting the Labels based on the validation features #####\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "######## Showing the list of features ranked based the the impact to the prediction ############\n",
    "show_features_impact(linearregression.coef_, X_train, header)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUMMARY FLOWCHART(Part 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image \n",
    "Image(filename='./images/Project_Flowchart.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
